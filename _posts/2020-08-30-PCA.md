---
layout: post
title: Principal Component Analysis
date: 2020-08-30
tags: Unsupervised Learning
---

## Unsupervised Dimentionality Reduction  
**via Principal Component Analysis**

In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information.  
Feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the curse of dimensionality—especially if we are working with non-regularized models.  

> ![PCA Index](https://github.com/WZHOU007-0912/images/raw/master/pca.jpg)
> $x_1$,$x_2$ are original feature axes, $pc_1$,$pc_2$ are the principal components.

**Principal Component Analysis (PCA)** aims to find the *directions* of **maximum variance** in high dimensional data and projects it onto a new subspace with fewer dimensions than original one.  
The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum given the constraints that new features are **orthogonal** to each other.  
(*orthogonal means uncorrelated*)

- What is an orthogonal matrix?  

Say we have:  
  
$$a_1^2 + a_2^2=1$$  
  
$$b_1^2 + b_2^2=1$$   
  
$$a_1 b_1 + a_2 b_2=0$$   
  
$$b_1 a_1 + b_2 a_2=0$$    
  
that is:  
  
$$\begin{bmatrix} a_1 & a_2\\ b_1 & b_2 \end{bmatrix} \begin{bmatrix} a_1 & b_1\\a_2 & b_2 \end{bmatrix} =
  \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} $$  
  
$$ for: A = \begin{bmatrix} a_1 & a_2\\ b_1 & b_2 \end{bmatrix}, {A^T}= \begin{bmatrix} a_1 & b_1\\ a_2 & b_2 \end{bmatrix}, I = \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}$$  
  
$$AA^T = I$$    
    
Then $A$ is an orthogonal matrix.
  


## Extracting the principal components step by step

### 1. Standardizing the data  
- PCA directions are highly sensitive to data scaling. We need to standardize the features **prior** to PCA.  
- We will start by loading the Wine datase from:
https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data 


```python
import pandas as pd

df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
'machine-learning-databases/wine/wine.data', header=None)

df_wine.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>



- The first row is the label and the rest are features.  
- Next, we process the Wine data into separate training and test sets—using 70 percent and 30 percent of the data.


```python
from sklearn.model_selection import train_test_split

X,y = df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
X_train, X_test, y_train, y_test = \
train_test_split(X,y,test_size = 0.3,
                stratify = y, 
                random_state = 0)
```

- Standardize the features


```python
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.fit_transform(X_test)
```

### 2. Constructing the covariance matrix  
- The covariance between two features x<sub>i</sub> and x<sub>j</sub> on population level can be calculated as:  
  
 $$\sigma_ij = \sum_{i=1}^{n}(x_j^{(i)} - \mu_j)(x_k^{(i)}-\mu_k)$$
  
  
- Since we already have standardized dataset, then covariance matrix can be calculated as:  
  
  $$\Sigma = \frac{1}{n}X X^T$$    
  
  Where $\Sigma$ is the covariance matrix of features, $X$ is the feature matrix.  
    
      
- **Why we need to calculate the covariance matrix for features?**
  
  The covariance matrix is calculating the correlation between the matrix.  
  The goal of PCA can be interpreted as maximize **the main diagonal** (the 'covariance' between the feature and itself, that is, the variance) while minimize the rest of the diagonals (the covariance between different features).  
    
      
- **Then why do we need to maximimze the variance?**  
  Note that variance is a measure of the "variability" of the data you have. When you extract the features from your dataset, you would want them to account for the most variability possible: hence the search for maximum variance, so that the *principal components* collect the most "uniqueness" from the data set.


```python
import numpy as np

# for numpy cov function, each row vector is a sample 
# we need to transpose the matrix so that
# the dataset is then become (n_samples, n_features)
con_mat = np.cov(X_train_std.T)
```

### 3. Obtaining the eigenvalues and eigenvectors 
**of the covariance matrix**

- Why eigenvalues/vectors?  
  PCA does not actually increase the variance of your data. Rather, it rotates the dataset in such a way as to align the directions in which it is spread out the most with the principal axes.  
  When you calculating the eigenvalues, you are actually trying to find a diagonal matrix which the entries outside **the main diagonal** are all zero:  
    
  $$\begin{bmatrix}
   \lambda_1 & 0 & 0 & ... & 0\\
   0 & \lambda_2 & 0 & ... & 0 \\
   0 & 0 & 0 &... & \lambda_n
  \end{bmatrix} $$    
  
  where $\lambda_i$ is the eigenvalue (a scalar) that defines the magnitude of the eigenvector. And eigenvectors represent the principal components (the direction of maximum variance).


```python
eigen_val, eigen_vec = np.linalg.eig(con_mat)
print('\nEigenvalues \n%s'%eigen_val)
```

    
    Eigenvalues 
    [4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
     0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
     0.1808613 ]


### 4. Sorting the eigenvalues 
**by decreasing order to rank the eigenvectors**  
  
Since we want to decrease the dimensionality of our dataset by compressing it into new feature subspace, we only select the subset of the eigenvectors(principal components) that contains most of the information(variance).  
Now let's plot the *variance explained ratios* of the eigenvalues:  
  
$$\frac{\lambda_i}{\sum_{i=1}^{n}\lambda_i}$$


```python
tot = sum(eigen_val)
history = [(i/tot) for i in
          sorted(eigen_val,reverse = True)]

import matplotlib.pyplot as plt
plt.bar(range(1,14),history, align = 'center', color = 'hotpink')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
```




    Text(0.5, 0, 'Principal component index')




![](https://github.com/WZHOU007-0912/images/raw/master/PCA%20index.png)


We can see that the first two principal components combined to explain almost 60% of the variance in the dataset.

### 5. Select K eigenvectors
**which correspond to K largest eigenvalues**  
  
K is the dimensionality of the new feature subspace. 


```python
eigen_pair = [(np.abs(eigen_val[i]),eigen_vec[:,i])
             for i in range(len(eigen_val))]

eigen_pair.sort(key = lambda k: k[0], reverse = True)
```

### 6. Construct a projection matrix W
**from K eigenvectors**  
  
Here we let k = 2 for convinience.  


```python
# np.hstack: split an array into 
# multiple sub-arrays horizontally (column-wise)
w = np.hstack((eigen_pair[0][1][:, np.newaxis], 
               eigen_pair[1][1][:, np.newaxis]))
print('Matrix W:\n',w)
```

    Matrix W:
     [[-0.13724218  0.50303478]
     [ 0.24724326  0.16487119]
     [-0.02545159  0.24456476]
     [ 0.20694508 -0.11352904]
     [-0.15436582  0.28974518]
     [-0.39376952  0.05080104]
     [-0.41735106 -0.02287338]
     [ 0.30572896  0.09048885]
     [-0.30668347  0.00835233]
     [ 0.07554066  0.54977581]
     [-0.32613263 -0.20716433]
     [-0.36861022 -0.24902536]
     [-0.29669651  0.38022942]]


### 7. Transform the features using matrix W
**to obtain the new K-dimensional feature subspace**  
  
$X^{'} = XW $


```python
# dot:inner product
X_train_pca = X_train_std.dot(w)
print(X_train_pca.shape)
```

    (124, 2)


- Now the transformed training set has been stored as 124 * 2-dimensionl matrix.  
  We can then visualize it in a 2-dimensional scatter plot.


```python
colors = ['plum','khaki','turquoise']
markers = ['s','x','o']
for l,c,m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_pca[y_train == l, 0],
                X_train_pca[y_train == l, 1],
                c = c, label = l, marker = m)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend(loc='lower left')
plt.show()
```


![](https://github.com/WZHOU007-0912/images/raw/master/output_28_0.png)


- We can see that the data spread more on pc1 (the first principal component) than pc2, which is consistent with the variance explained ratios that we've calculated in section 4.

---------------
# PCA in scikit-learn

**1. PAC transform the dataset**   
  
**2. Classify the transformed samples via logistic regression**  


```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_train_pca_sk = pca.fit_transform(X_train_std)
```


```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train_pca_sk,y_train)
```




    LogisticRegression()



**3. Visualize the decision regions**


```python
from matplotlib.colors import ListedColormap

def plot_decision_regions(X, y, classifier, resolution=0.02):
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('lightpink', 'slateblue', 'gold', 'lightgreen', 'lightcyan') 
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution)) 
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) 
    plt.xlim(xx1.min(), xx1.max()) 
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    alpha=0.6, c=cmap(idx), edgecolor='black', 
                    marker=markers[idx], label=cl)

```


```python
from matplotlib.axes._axes import _log as matplotlib_axes_logger
matplotlib_axes_logger.setLevel('ERROR')

plot_decision_regions(X_train_pca_sk, y_train, classifier=lr)
```


![](https://github.com/WZHOU007-0912/images/raw/master/output_36_0.png)


Now let's plot the decision regions of the logistic regression on the transformed test dataset.


```python
# transform() : parameters generated from fit() method,
# applied upon model to generate transformed data set.
X_test_pca_sk = pca.transform(X_test_std)

plot_decision_regions(X_test_pca_sk, y_test, classifier=lr)
```


![](https://github.com/WZHOU007-0912/images/raw/master/output_38_0.png)


The result indicates that logistic regression performs quite well on this two-dimensional feature subspace.

---------------
# References
1. Python Machine Learning - Second Edition  
2. https://stackoverflow.com/questions/12395542/why-do-we-maximize-variance-during-principal-component-analysis
3. https://www.cnblogs.com/LittleHann/p/10859016.html#_label2

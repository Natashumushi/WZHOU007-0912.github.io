<!DOCTYPE html>
<html>

  <head>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$','$']]
          }
      });
  </script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Principal Component Analysis</title>
  <meta name="description" content="Unsupervised Dimentionality Reductionvia Principal Component Analysis">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Principal Component Analysis">
  <meta name="twitter:description" content="Unsupervised Dimentionality Reductionvia Principal Component Analysis">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Principal Component Analysis">
  <meta property="og:description" content="Unsupervised Dimentionality Reductionvia Principal Component Analysis">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/2020/08/PCA/">
  <link rel="alternate" type="application/rss+xml" title="Zhou Wei" href="http://localhost:4000/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="ÂâçÂæÄ Zhou Wei ÁöÑ‰∏ªÈ°µ" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Zhou Wei logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Zhou Wei" class="blog-button">Zhou Wei</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">My journey to data science üëßüèΩ</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">ta-da! welcome to my blog!</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/natasumushi" title="@natasumushi ÁöÑÂæÆÂçö" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/WZHOU007-0912" title="@WZHOU007-0912 ÁöÑ Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  

  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:WZHOU007@e.ntu.edu.sg" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-disabled"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2020-08-30 00:00:00 +0800" itemprop="datePublished" class="post-meta__date date">2020-08-30</time> &#8226; <span class="post-meta__tags tags">Unsupervised Learning</span>
    </div>
    <h1 class="post-title">Principal Component Analysis</h1>
  </header>

  <section class="post">
    <h2 id="unsupervised-dimentionality-reduction">Unsupervised Dimentionality Reduction</h2>
<p><strong>via Principal Component Analysis</strong></p>

<p>In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information.<br />
Feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the curse of dimensionality‚Äîespecially if we are working with non-regularized models.</p>

<blockquote>
  <p><img src="https://github.com/WZHOU007-0912/images/raw/master/pca.jpg" alt="PCA Index" />
$x_1$,$x_2$ are original feature axes, $pc_1$,$pc_2$ are the principal components.</p>
</blockquote>

<p><strong>Principal Component Analysis (PCA)</strong> aims to find the <em>directions</em> of <strong>maximum variance</strong> in high dimensional data and projects it onto a new subspace with fewer dimensions than original one.<br />
The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum given the constraints that new features are <strong>orthogonal</strong> to each other.<br />
(<em>orthogonal means uncorrelated</em>)</p>

<ul>
  <li>What is an orthogonal matrix?</li>
</ul>

<p>Say we have:</p>

<script type="math/tex; mode=display">a_1^2 + a_2^2=1</script>

<script type="math/tex; mode=display">b_1^2 + b_2^2=1</script>

<script type="math/tex; mode=display">a_1 b_1 + a_2 b_2=0</script>

<script type="math/tex; mode=display">b_1 a_1 + b_2 a_2=0</script>

<p>that is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix} a_1 & a_2\\ b_1 & b_2 \end{bmatrix} \begin{bmatrix} a_1 & b_1\\a_2 & b_2 \end{bmatrix} =
  \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
for: A = \begin{bmatrix} a_1 & a_2\\ b_1 & b_2 \end{bmatrix}, {A^T}= \begin{bmatrix} a_1 & b_1\\ a_2 & b_2 \end{bmatrix}, I = \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} %]]></script>

<script type="math/tex; mode=display">AA^T = I</script>

<p>Then $A$ is an orthogonal matrix.</p>

<h2 id="extracting-the-principal-components-step-by-step">Extracting the principal components step by step</h2>

<h3 id="1-standardizing-the-data">1. Standardizing the data</h3>
<ul>
  <li>PCA directions are highly sensitive to data scaling. We need to standardize the features <strong>prior</strong> to PCA.</li>
  <li>We will start by loading the Wine datase from:
https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">df_wine</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://archive.ics.uci.edu/ml/'</span>
<span class="s">'machine-learning-databases/wine/wine.data'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">df_wine</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
  <li>The first row is the label and the rest are features.</li>
  <li>Next, we process the Wine data into separate training and test sets‚Äîusing 70 percent and 30 percent of the data.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">df_wine</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">df_wine</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
                <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> 
                <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Standardize the features</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_std</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_std</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="2-constructing-the-covariance-matrix">2. Constructing the covariance matrix</h3>
<ul>
  <li>The covariance between two features x<sub>i</sub> and x<sub>j</sub> on population level can be calculated as:</li>
</ul>

<script type="math/tex; mode=display">\sigma_ij = \sum_{i=1}^{n}(x_j^{(i)} - \mu_j)(x_k^{(i)}-\mu_k)</script>

<ul>
  <li>
    <p>Since we already have standardized dataset, then covariance matrix can be calculated as:</p>

    <script type="math/tex; mode=display">\Sigma = \frac{1}{n}X X^T</script>

    <p>Where $\Sigma$ is the covariance matrix of features, $X$ is the feature matrix.</p>
  </li>
  <li>
    <p><strong>Why we need to calculate the covariance matrix for features?</strong></p>

    <p>The covariance matrix is calculating the correlation between the matrix.<br />
The goal of PCA can be interpreted as maximize <strong>the main diagonal</strong> (the ‚Äòcovariance‚Äô between the feature and itself, that is, the variance) while minimize the rest of the diagonals (the covariance between different features).</p>
  </li>
  <li>
    <p><strong>Then why do we need to maximimze the variance?</strong><br />
Note that variance is a measure of the ‚Äúvariability‚Äù of the data you have. When you extract the features from your dataset, you would want them to account for the most variability possible: hence the search for maximum variance, so that the <em>principal components</em> collect the most ‚Äúuniqueness‚Äù from the data set.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># for numpy cov function, each row vector is a sample 
# we need to transpose the matrix so that
# the dataset is then become (n_samples, n_features)
</span><span class="n">con_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_train_std</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="3-obtaining-the-eigenvalues-and-eigenvectors">3. Obtaining the eigenvalues and eigenvectors</h3>
<p><strong>of the covariance matrix</strong></p>

<ul>
  <li>
    <p>Why eigenvalues/vectors?<br />
PCA does not actually increase the variance of your data. Rather, it rotates the dataset in such a way as to align the directions in which it is spread out the most with the principal axes.<br />
When you calculating the eigenvalues, you are actually trying to find a diagonal matrix which the entries outside <strong>the main diagonal</strong> are all zero:</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}
 \lambda_1 & 0 & 0 & ... & 0\\
 0 & \lambda_2 & 0 & ... & 0 \\
 0 & 0 & 0 &... & \lambda_n
\end{bmatrix} %]]></script>

    <p>where $\lambda_i$ is the eigenvalue (a scalar) that defines the magnitude of the eigenvector. And eigenvectors represent the principal components (the direction of maximum variance).</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eigen_val</span><span class="p">,</span> <span class="n">eigen_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">con_mat</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Eigenvalues </span><span class="se">\n</span><span class="si">%</span><span class="s">s'</span><span class="o">%</span><span class="n">eigen_val</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Eigenvalues 
[4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
 0.1808613 ]
</code></pre></div></div>

<h3 id="4-sorting-the-eigenvalues">4. Sorting the eigenvalues</h3>
<p><strong>by decreasing order to rank the eigenvectors</strong></p>

<p>Since we want to decrease the dimensionality of our dataset by compressing it into new feature subspace, we only select the subset of the eigenvectors(principal components) that contains most of the information(variance).<br />
Now let‚Äôs plot the <em>variance explained ratios</em> of the eigenvalues:</p>

<script type="math/tex; mode=display">\frac{\lambda_i}{\sum_{i=1}^{n}\lambda_i}</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tot</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">eigen_val</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="o">/</span><span class="n">tot</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
          <span class="nb">sorted</span><span class="p">(</span><span class="n">eigen_val</span><span class="p">,</span><span class="n">reverse</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)]</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">14</span><span class="p">),</span><span class="n">history</span><span class="p">,</span> <span class="n">align</span> <span class="o">=</span> <span class="s">'center'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'hotpink'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Explained variance ratio'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Principal component index'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'Principal component index')
</code></pre></div></div>

<p><img src="https://github.com/WZHOU007-0912/images/raw/master/PCA%20index.png" alt="" /></p>

<p>We can see that the first two principal components combined to explain almost 60% of the variance in the dataset.</p>

<h3 id="5-select-k-eigenvectors">5. Select K eigenvectors</h3>
<p><strong>which correspond to K largest eigenvalues</strong></p>

<p>K is the dimensionality of the new feature subspace.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eigen_pair</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">eigen_val</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">eigen_vec</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigen_val</span><span class="p">))]</span>

<span class="n">eigen_pair</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="n">k</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="6-construct-a-projection-matrix-w">6. Construct a projection matrix W</h3>
<p><strong>from K eigenvectors</strong></p>

<p>Here we let k = 2 for convinience.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># np.hstack: split an array into 
# multiple sub-arrays horizontally (column-wise)
</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">eigen_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> 
               <span class="n">eigen_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Matrix W:</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Matrix W:
 [[-0.13724218  0.50303478]
 [ 0.24724326  0.16487119]
 [-0.02545159  0.24456476]
 [ 0.20694508 -0.11352904]
 [-0.15436582  0.28974518]
 [-0.39376952  0.05080104]
 [-0.41735106 -0.02287338]
 [ 0.30572896  0.09048885]
 [-0.30668347  0.00835233]
 [ 0.07554066  0.54977581]
 [-0.32613263 -0.20716433]
 [-0.36861022 -0.24902536]
 [-0.29669651  0.38022942]]
</code></pre></div></div>

<h3 id="7-transform-the-features-using-matrix-w">7. Transform the features using matrix W</h3>
<p><strong>to obtain the new K-dimensional feature subspace</strong></p>

<p>$X^{‚Äò} = XW $</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># dot:inner product
</span><span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">X_train_std</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(124, 2)
</code></pre></div></div>

<ul>
  <li>Now the transformed training set has been stored as 124 * 2-dimensionl matrix.<br />
We can then visualize it in a 2-dimensional scatter plot.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'plum'</span><span class="p">,</span><span class="s">'khaki'</span><span class="p">,</span><span class="s">'turquoise'</span><span class="p">]</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s">'s'</span><span class="p">,</span><span class="s">'x'</span><span class="p">,</span><span class="s">'o'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="n">colors</span><span class="p">,</span> <span class="n">markers</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                <span class="n">X_train_pca</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">l</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">l</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="n">m</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'PC 1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PC 2'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'lower left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://github.com/WZHOU007-0912/images/raw/master/output_28_0.png" alt="" /></p>

<ul>
  <li>We can see that the data spread more on pc1 (the first principal component) than pc2, which is consistent with the variance explained ratios that we‚Äôve calculated in section 4.</li>
</ul>

<hr />
<h1 id="pca-in-scikit-learn">PCA in scikit-learn</h1>

<p><strong>1. PAC transform the dataset</strong></p>

<p><strong>2. Classify the transformed samples via logistic regression</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_train_pca_sk</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca_sk</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LogisticRegression()
</code></pre></div></div>

<p><strong>3. Visualize the decision regions</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span>
    <span class="c1"># setup marker generator and color map
</span>    <span class="n">markers</span> <span class="o">=</span> <span class="p">(</span><span class="s">'s'</span><span class="p">,</span> <span class="s">'x'</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="s">'^'</span><span class="p">,</span> <span class="s">'v'</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">(</span><span class="s">'lightpink'</span><span class="p">,</span> <span class="s">'slateblue'</span><span class="p">,</span> <span class="s">'gold'</span><span class="p">,</span> <span class="s">'lightgreen'</span><span class="p">,</span> <span class="s">'lightcyan'</span><span class="p">)</span> 
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>
    
    <span class="c1"># plot the decision surface
</span>    <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span> 
    <span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span> 
    <span class="n">Z</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">xx1</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">xx2</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
    
    <span class="c1"># plot class samples
</span>    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> 
                    <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">cl</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib.axes._axes</span> <span class="kn">import</span> <span class="n">_log</span> <span class="k">as</span> <span class="n">matplotlib_axes_logger</span>
<span class="n">matplotlib_axes_logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="s">'ERROR'</span><span class="p">)</span>

<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_train_pca_sk</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://github.com/WZHOU007-0912/images/raw/master/output_36_0.png" alt="" /></p>

<p>Now let‚Äôs plot the decision regions of the logistic regression on the transformed test dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># transform() : parameters generated from fit() method,
# applied upon model to generate transformed data set.
</span><span class="n">X_test_pca_sk</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>

<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X_test_pca_sk</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://github.com/WZHOU007-0912/images/raw/master/output_38_0.png" alt="" /></p>

<p>The result indicates that logistic regression performs quite well on this two-dimensional feature subspace.</p>

<hr />
<h1 id="references">References</h1>
<ol>
  <li>Python Machine Learning - Second Edition</li>
  <li>https://stackoverflow.com/questions/12395542/why-do-we-maximize-variance-during-principal-component-analysis</li>
  <li>https://www.cnblogs.com/LittleHann/p/10859016.html#_label2</li>
</ol>

  </section>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">Recent</span>
       <h2 class="post-list__post-title post-title"><a href="/2020/09/Autoencoder/" title="link to scRNA-seq Data Analysis">scRNA-seq Data Analysis</a></h2>
       <p class="excerpt">0. IntroductionThis kernel uses the data from Tabula Muris, follows the course developed by the Hemberg Lab and the Computational Biology team at the Chan Zuckerberg Initiative for data preprocessing, builds an Autoencoder (in Keras) + t-SNE for d...&hellip;</p>
       <div class="post-list__meta"><time datetime="2020-09-21 00:00:00 +0800" class="post-list__meta--date date">2020-09-21</time> &#8226; <span class="post-list__meta--tags tags">Unsupervised Learning</span><a class="btn-border-small" href=/2020/09/Autoencoder/>Read more</a></div>
   </div>
   
   
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://localhost:4000/2020/08/PCA/";
        this.page.identifier = "/2020/08/PCA/";
    };

    var disqus_shortname = 'natasumushi';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Ë¶ÅÊü•Áúã<a href="http://disqus.com/?ref_noscript"> Disqus </a>ËØÑËÆ∫ÔºåËØ∑ÂêØÁî® JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">Êú¨Á´ôÁÇπÈááÁî®<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Áü•ËØÜÂÖ±‰∫´ ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Áõ∏ÂêåÊñπÂºèÂÖ±‰∫´ 4.0 ÂõΩÈôÖ ËÆ∏ÂèØÂçèËÆÆ</a></span>
        <span class="footer__copyright">Áî± <a href="https://jekyllrb.com">Jekyll</a> ‰∫é 2020-09-21 ÁîüÊàêÔºåÊÑüË∞¢ <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> ‰∏∫Êú¨Á´ôÊèê‰æõÁ®≥ÂÆöÁöÑ VPS ÊúçÂä°</span>
        <span class="footer__copyright">Êú¨Á´ôÈááÁî® <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> ‰Ωú‰∏∫‰∏ªÈ¢òÔºåÊÇ®ÂèØ‰ª•Âú® GitHub ÊâæÂà∞<a href="https://github.com/onevcat/OneV-s-Den">Êú¨Á´ôÊ∫êÁ†Å</a> - &copy; 2020</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



  
    
  </body>

</html>

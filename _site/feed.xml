<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhou Wei</title>
    <description>ta-da! welcome to my blog!</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 30 Aug 2020 20:56:55 +0800</pubDate>
    <lastBuildDate>Sun, 30 Aug 2020 20:56:55 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Principle Component Analysis</title>
        <description>&lt;h1 id=&quot;unsupervised-dimentionality-reduction&quot;&gt;Unsupervised Dimentionality Reduction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;via Principle Component Analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information.&lt;br /&gt;
Feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the curse of dimensionality—especially if we are working with non-regularized models.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;1&lt;/sub&gt; are original feature axes, pc&lt;sub&gt;1&lt;/sub&gt;,pc&lt;sub&gt;2&lt;/sub&gt; are the principle components.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Principle Component Analysis (PCA)&lt;/strong&gt; aims to find the &lt;em&gt;directions&lt;/em&gt; of &lt;strong&gt;maximum variance&lt;/strong&gt; in high dimensional data and projects it onto a new subspace with fewer dimensions than original one.&lt;br /&gt;
The orthogonal axes (principle components) of the new subspace can be interpreted as the directions of maximum given the constraints that new features are &lt;strong&gt;orthogonal&lt;/strong&gt; to each other.&lt;br /&gt;
(&lt;em&gt;orthogonal means uncorrelated&lt;/em&gt;)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What is an orthogonal matrix?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Say we have:&lt;/p&gt;

&lt;p&gt;a&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; + a&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;=1&lt;br /&gt;
  b&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; + b&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;=1&lt;br /&gt;
  a&lt;sub&gt;1&lt;/sub&gt;b&lt;sub&gt;1&lt;/sub&gt;+a&lt;sub&gt;2&lt;/sub&gt;b&lt;sub&gt;2&lt;/sub&gt;=0&lt;br /&gt;
  b&lt;sub&gt;1&lt;/sub&gt;a&lt;sub&gt;1&lt;/sub&gt;+b&lt;sub&gt;2&lt;/sub&gt;a&lt;sub&gt;2&lt;/sub&gt;=0&lt;/p&gt;

&lt;p&gt;that is:&lt;/p&gt;

&lt;p&gt;$\begin{bmatrix}
   a_1 &amp;amp; a_2&lt;br /&gt;
   b_1 &amp;amp; b_2 &lt;br /&gt;
  \end{bmatrix} $ 
  $\begin{bmatrix}
   a_1 &amp;amp; b_1&lt;br /&gt;
   a_2 &amp;amp; b_2 &lt;br /&gt;
  \end{bmatrix} $
  =
  $\begin{bmatrix}
   1 &amp;amp; 0&lt;br /&gt;
   0 &amp;amp; 1 &lt;br /&gt;
  \end{bmatrix} $&lt;/p&gt;

&lt;p&gt;for A =  $\begin{bmatrix}
   a_1 &amp;amp; a_2&lt;br /&gt;
   b_1 &amp;amp; b_2 &lt;br /&gt;
  \end{bmatrix} $,  A&lt;sup&gt;’&lt;/sup&gt; = $\begin{bmatrix}
   a_1 &amp;amp; b_1&lt;br /&gt;
   a_2 &amp;amp; b_2 &lt;br /&gt;
  \end{bmatrix} $&lt;/p&gt;

&lt;p&gt;AA&lt;sup&gt;’&lt;/sup&gt; = I&lt;/p&gt;

&lt;p&gt;Then A is an orthogonal matrix.&lt;/p&gt;

&lt;h1 id=&quot;extracting-the-principle-components-step-by-step&quot;&gt;Extracting the principle components step by step&lt;/h1&gt;

&lt;h1 id=&quot;1standardizing-the-data&quot;&gt;1.Standardizing the data&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;PCA directions are highly sensitive to data scaling. We need to standardize the features &lt;strong&gt;prior&lt;/strong&gt; to PCA.&lt;/li&gt;
  &lt;li&gt;We will start by loading the Wine datase from:
https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;import pandas as pd&lt;/p&gt;

&lt;p&gt;df_wine = pd.read_csv(‘https://archive.ics.uci.edu/ml/’
‘machine-learning-databases/wine/wine.data’, header=None)&lt;/p&gt;

&lt;p&gt;df_wine.head(5)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first row is the label and the rest are features.&lt;/li&gt;
  &lt;li&gt;Next, we process the Wine data into separate training and test sets—using 70 percent and 30 percent of the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;stratify&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Standardize the features&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;2-constructing-the-covariance-matrix&quot;&gt;2. Constructing the covariance matrix&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The covariance between two features x&lt;sub&gt;i&lt;/sub&gt; and x&lt;sub&gt;j&lt;/sub&gt; on population level can be calculated as:&lt;/p&gt;

    &lt;p&gt;$\sigma$&lt;sub&gt;ij&lt;/sub&gt; = $\sum_{i=1}^{n} (x$&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(i)&lt;/sup&gt; - $\mu$&lt;sub&gt;j&lt;/sub&gt;)$(x$&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;(i)&lt;/sup&gt; - $\mu$&lt;sub&gt;k&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since we already have standardized dataset, then covariance matrix can be calculated as:&lt;/p&gt;

    &lt;p&gt;$\Sigma$ = $\frac{1}{n}$X X&lt;sup&gt;T&lt;/sup&gt;&lt;/p&gt;

    &lt;p&gt;Where $\Sigma$ is the covariance matrix of features, X is the feature matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Why we need to calculate the covariance matrix for features?&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The covariance matrix is calculating the correlation between the matrix.&lt;br /&gt;
The goal of PCA can be interpreted as maximize the main diagonal (the covariance of feature and itself) while minimize  the rest of the diagonals (the covariance between different features).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 30 Aug 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/08/PCA/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/08/PCA/</guid>
        
        <category>Unsupervised</category>
        
        <category>Learning</category>
        
        
      </item>
    
  </channel>
</rss>

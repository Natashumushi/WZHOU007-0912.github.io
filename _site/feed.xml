<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhou Wei</title>
    <description>ta-da! welcome to my blog!</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 30 Aug 2020 20:01:20 +0800</pubDate>
    <lastBuildDate>Sun, 30 Aug 2020 20:01:20 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Principle Component Analysis Tutorial</title>
        <description>&lt;h1 id=&quot;unsupervised-dimentionality-reduction&quot;&gt;Unsupervised Dimentionality Reduction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;via Principle Component Analysis&lt;/strong&gt;
In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. &lt;br /&gt;
Feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the curse of dimensionalityâ€”especially if we are working with non-regularized models.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;attachment:Screenshot%202020-08-30%20at%2012.30.54%20AM.png&quot; alt=&quot;Screenshot%202020-08-30%20at%2012.30.54%20AM.png&quot; /&gt;
x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;1&lt;/sub&gt; are original feature axes, pc&lt;sub&gt;1&lt;/sub&gt;,pc&lt;sub&gt;2&lt;/sub&gt; are the principle components.&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Sun, 30 Aug 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/08/Test/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/08/Test/</guid>
        
        <category>Machine</category>
        
        <category>Learning</category>
        
        
      </item>
    
  </channel>
</rss>
